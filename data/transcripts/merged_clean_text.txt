--- Databases for GenAI.pdf ---
1
Place for photo
Databases for 
GenAI
Embeddings, Vector Databases & Production 
Patterns
October 2025
2
Speaker
Maksym Lypivskyi
Head of Cloud Platforms & 
AI Director
‚óè
9 years at Ciklum driving large-scale cloud and 
software delivery initiatives
‚óè
3 years specializing in AI
‚óè
Core interest: making AI systems reliable, 
production-ready, and business-impactful
3
Agenda
01
02
03
Foundations
Advanced RAG patterns
Production readiness
4
Foundations
5
What Are Embeddings?
The Problem: Computers don't understand text meaning
"machine learning 
algorithms"
"ML techniques"
"deep neural networks"
Are these similar? How 
do we compute that?
6
How Similarity Search Works
The Vector Search Process
Documents ‚Üí Embedding Model ‚Üí Store vectors
Doc 1: "Python tutorial" ‚Üí [0.2, -0.4, 0.6, ...] ‚Üí Store
Doc 2: "Java programming" ‚Üí [0.3, -0.5, 0.5, ...] ‚Üí Store
Doc 3: "Cooking recipes" ‚Üí [-0.8, 0.9, -0.2, ...] ‚Üí Store
1. INDEXING
2. SEARCHING
 User Query: "learn Python" ‚Üí [0.21, -0.42, 0.58, ...]
 Compute distances to ALL stored vectors:
 - Distance to Doc 1: 0.03 ‚úì (very close!)
 - Distance to Doc 2: 0.15 (somewhat close)
 - Distance to Doc 3: 0.91 (far away)
Similarity 
Metrics
- Cosine similarity: Angle between vectors (most common)
- Euclidean distance: Straight-line distance
- Dot product: Vector multiplication
7
Embedding Quality Matters
üèõ EVERLAW (Legal 
Discovery - 1.4M 
documents)
https://huggingface.co/spaces/mteb/leaderboard
Model
Provider
MTEB
Dims
Price/1M
stella_en_1.5B_v5
NovaSearch
71.54
1024-8192
Free (OSS)
gemini-embedding-001
Google
68.32
3072
$0.15
text-embedding-3-large
OpenAI
64.6
3072
$0.13
text-embedding-3-small
OpenAI
62.3
1536
$0.02
üí¨ MINDLID
üìß INTERACTION CO 
(Email Assistant - 100 
emails)
87% 
accuracy
82% 
3 recall
21.45s 
embedd
8
PostgreSQL Revolution
9
Architecture & Performance
- Rust-based for maximum speed
- 626 QPS at 99.5% recall (1M vectors)
- Sub-3ms p95 latency
- Open-source (Apache 2.0 license)
Memory Optimization: 
- Binary Quantization: 40x memory reduction - 
Scalar quantization 
- Product quantization
Advanced Capabilities:
- Native hybrid search (dense + sparse 
vectors)
- Advanced payload Ô¨Åltering
- Distributed architecture for horizontal 
scaling
- RESTful and gRPC APIs
From MVP to Scale
Mid Size High-Performance Vector Search
Architecture & Performance
- Built for massive scale (billions of vectors)
- 2,098 QPS at 100% recall (10M vectors)
- Sub-10ms latency on performance 
conÔ¨Ågurations
- Open-source (Apache 2.0 license)
Memory Optimization: 
- Int8 compression: 75% memory savings
- RabitQ 1-bit quantization
- Multiple quantization strategies
Advanced Capabilities:
- GPU acceleration support
- Up to 100,000 collections per cluster
- Distributed architecture with query node 
separation
- Multiple distance metrics and index types 
(HNSW, IVF, DiskANN)
Architecture & Performance
- Embedded database (runs in-process with 
your application)
- 112 QPS at 10M vectors
- Python and JavaScript SDKs
- Open-source (Apache 2.0 license)
Memory Optimization: 
- In-memory and persistent storage modes
- Optimized for datasets under 1M vectors
- Simple distance functions (cosine, L2, inner 
product)
Advanced Capabilities:
- Zero-conÔ¨Åguration setup (pip install 
chromadb)
- Metadata Ô¨Åltering
- Multiple collection support
10
Advanced RAG patterns
11
11
The Multi-Hop Reasoning Problem
Complex Query:
"What are the regulatory implications of our Q3 marketing 
strategy for the European market?"
Traditional RAG fails
Retrieves isolated chunks:
üÜá
- "Q3 marketing strategy increased 
social media spend"
üÜá
- "European market regulations 
overview" 
üÜá
- "GDPR compliance requirements"
Problem: Can't connect the dots!
Marketing ‚Üí Budget ‚Üí Compliance ‚Üí GDPR ‚Üí Europe
This is where we need Graph RAG
12
12
GraphRAG Solution
13
13
The PDF Processing Problem
üÜá
OCR ‚Üí Errors with complex layouts
üÜá
Layout detection ‚Üí Misses table structures
üÜá
Figure captioning ‚Üí Expensive specialized 
models
üÜá
Chunking ‚Üí Loses visual context
üÜá
Embed text only ‚Üí No visual information
Traditional RAG Pipeline for PDFs
Result: 40-60% information loss on 
visual documents
14
14
ColPali Revolution
Treat Pages as Images (No OCR!)
Architecture
‚óè
PaliGemma-3B Vision Language Model
‚óè
32√ó32 patches = 1,024 patch 
embeddings per page
‚óè
ColBERT late interaction matching
15
15
What Makes RAG "Agentic"
16
16
Agentic Patterns
Query Decomposition
Complex: "Compare our Q3 to 
industry and predict Q4"
Agent breaks down:
‚óè
"Our Q3 metrics" ‚Üí Internal DB
‚óè
"Industry Q3 benchmarks" ‚Üí 
Web Search
‚óè
"Q4 factors" ‚Üí Internal DB
Then synthesizes all results
Self-Correction
1.
Retrieval
2.
Grade docs
3.
Score < threshold?
4.
Rewrite query 
5.
Retry
6.
Grade again
Query Routing
User: "What's the latest on AI 
regulation?"
Router: "latest" detected ‚Üí Route 
to Web Search ‚úì
User: "What's our Q3 marketing 
budget?" 
Router: "our" = internal ‚Üí Route to 
Vector DB ‚úì
‚úÖ
Use When: Queries span sources, complexity varies
üÜá
Don't Use: Latency-critical (<500ms SLA), simple retrieval
17
17
Hybrid Search + Reranking Power
Vector-only: Misses exact matches, acronyms
BM25-only: No semantic understanding
Anthropic Contextual Retrieval 
Study:
- Baseline failure rate: 5.7%
- Hybrid + Reranker: 1.9%
- 67% reduction in retrieval failures
2 Stage Architecture Rerank:
Stage 1: 
Similarity search ‚Üí Top 100 
Stage 2: 
Re-ranking pipeline ‚Üí Top 5-10 
Microsoft Azure AI Search 
Benchmarks:
- Vector-only: 43.8 NDCG@3
- Hybrid: 48.4 NDCG@3 (+10.5%)
- Hybrid + Reranker: 60.1 NDCG@3 
(+37.2%)
‚úÖ
Use When: Queries span sources, complexity varies
üÜá
Don't Use: Latency-critical (<500ms SLA), simple retrieval
18
Production readiness
19
19
Don‚Äôts
üÜá
1. Vector-only search (use hybrid)
üÜá
2. Ignore access controls (data leakage = 
lawsuit)
üÜá
3. Overload context window (keep <50%)
üÜá
4. Skip evaluation frameworks (doesn't 
scale)
üÜá
5. Neglect data freshness (stale = wrong 
answers)
Do‚Äôs
‚úÖ
1. Use Hybrid Search (vector + BM25) as 
baseline
‚úÖ
2. Implement metadata Ô¨Åltering 
(security-critical)
‚úÖ
 Ô¨Ålter = {"department": "Ô¨Ånance", 
"access_level": user.role}
‚úÖ
3. Monitor retrieval quality (recall@k, 
NDCG)
‚úÖ
 Tools: TruLens, LangSmith, DeepEval
‚úÖ
4. Keep embeddings fresh (re-index on 
doc changes)
‚úÖ
5. Evaluate systematically (not 
"vibes-based")
Production Best Practices
--- RAG Intro.pdf ---
1
Place for photo
RAG
Intro
October 2025
2
Speaker
Maksym Lypivskyi
Head of Cloud Platforms & 
AI Director
‚óè
9 years at Ciklum driving large-scale cloud and 
software delivery initiatives
‚óè
3 years specializing in AI
‚óè
Core interest: making AI systems reliable, 
production-ready, and business-impactful
3
Agenda
01
02
03
DeÔ¨Ånition & BeneÔ¨Åts
Use cases
Limitations
4
The AI Adaptation Landscape
Where Does RAG Fit in Your AI Strategy?
‚óè
Need fresh, proprietary knowledge
‚óè
Don't want to retrain models
‚óè
Cost-effective scaling
Our focus today
5
What is RAG?
Retrieval‚Äëaugmented generation (RAG) is a 
pattern that augments an LLM by retrieving 
relevant information from external sources at 
query time and injecting it into the prompt.
6
Why RAG Matters?
‚óè
Fresh Additional Knowledge. RAG lets 
you ground answers in recent 
documents, internal wikis, or databases 
without retraining models.
‚óè
Better Accuracy. By retrieving 
authoritative evidence, the model 
generates more accurate answers and 
can cite sources.
‚óè
Adaptable. Effectively handles novel and 
niche queries that weren't in the model's 
training data.
‚óè
Increases EfÔ¨Åciency. RAG grounds 
prompts with smaller, targeted chunks of 
information that streamline retrieval and 
generation.
7
RAG Pipeline Deep Dive
8
Chunking - The Critical Decision
Why Chunking Matters
LLMs have limited context windows (32k-200k 
tokens). Large documents must be divided into 
smaller pieces.
The Challenge:
‚óè
Too large ‚Üí loses specificity, poor retrieval
‚óè
Too small ‚Üí loses context, fragmented 
information
9
Three Main Chunking Strategies
Strategy
How It Works
Pros
Cons
When to Use
Fixed-Size
Split at 512 
tokens, 15% 
overlap
Simple, fast, 
predictable
May break 
mid-sentence
General purpose, 
starting point
Semantic
Use ML to 
identify coherent 
units
Preserves 
meaning, high 
accuracy
More compute, 
slower
Technical docs, 
complex content
Recursive
Split using 
separators (\n\n, 
\n, space) 
repeatedly until 
desired size
Respects 
structure, better 
boundaries
More complex 
than Ô¨Åxed-size
Documents with 
headings, 
paragraphs
10
10
Chunking Strategy Comparison
Fixed-Size
Recursive
Semantic
11
11
Using Chunking Libraries
You Don't Need to Build from Scratch
LangChain
LlamaIndex
Broad LLM application 
framework
Modular workÔ¨Çows where 
chunking is one piece of the 
puzzle
‚óèFlexible TextSplitters
‚óèEasy integration with agents
‚óèPart of larger system
RAG-speciÔ¨Åc pipeline
High-performance, 
data-centric retrieval systems
‚óèSophisticated NodeParsers
‚óèProduces optimized "Nodes"
‚óèBuilt for ingestion/retrieval
12
12
RAG real world use cases
‚óè
AI Chatbots: RAG provides accurate answers from internal 
knowledge bases (e.g., support wikis, legal documents). OpenAI 
emphasises that RAG is valuable when the content is not part of 
the base model‚Äôs knowledge .
‚óè
Search & discovery: Search systems combine keyword and 
vector search to surface relevant documents in e‚Äëcommerce, 
research and legal discovery.
‚óè
AI Copilots: Tools like Supabase AI Copilots use vector 
databases to ground responses in proprietary data and maintain 
multi‚Äëtenant isolation .
‚óè
Long‚Äëcontext reasoning: Databricks‚Äô long‚Äëcontext benchmark 
shows that Google‚Äôs Gemini 2.5 models can maintain consistent 
performance on RAG tasks up to two million tokens (longer than 
most models), whereas OpenAI‚Äôs GPT 5 models achieve 
state‚Äëof‚Äëthe‚Äëart accuracy up to 128k tokens .
13
13
Common Challenges
‚óè
Chunking & context windows: If chunks are poorly deÔ¨Åned, the retrieved information may miss critical context 
or include too much irrelevant text. Research by Analytics Vidhya notes that Ô¨Åxed‚Äësize chunking can break 
context while semantic‚Äëbased chunking preserves meaning but requires more compute .
‚óè
Model context length: Models can only ingest a Ô¨Ånite number of tokens. Databricks‚Äô benchmark observed that 
performance of LLMs like Llama‚Äë3.1 and GPT‚Äë4 starts to degrade when context windows exceed 32‚Äì64 k tokens 
.
‚óè
Retrieval quality: The quality of the vector database and retrieval algorithm determines recall. Missing relevant 
documents leads to hallucinated answers.
‚óè
Latency & cost: Large vector databases and embedding models can be expensive and introduce latency.
14
Don‚Äôts
üÜá
Rely solely on vector search
üÜá
Ignore security and access controls
üÜá
Overload the LLM context window
üÜá
Neglect continuous updates
üÜá
Skip evaluation frameworks
Do‚Äôs
‚úÖ
Start simple, iterate based on metrics
‚úÖ
Use metadata Ô¨Åltering (product, language, 
permissions)
‚úÖ
Combine vector + keyword search (hybrid approach)
‚úÖ
Monitor retrieval quality (recall@k, precision)
‚úÖ
Keep embeddings synchronized with documents
‚úÖ
Evaluate with domain-speciÔ¨Åc questions
Do‚Äôs and don‚Äôts for RAG
15
15
Three Main Chunking Strategies
Scenario
Stack
Key Reason
Learning
LangChain + Chroma + 
sentence-transformers + Ollama
‚óè
Learn fundamentals
‚óè
Risk-free 
‚óè
Runs on laptop
MVP
LangChain + Qdrant (self-host) + 
OpenAI/Gemini embeddings + GPT-4o-mini
Professional quality at 
startup budget
Enterprise
LangGraph + LangSmith + Pinecone + 
OpenAI/Gemini + GPT-4
‚óè
Agentic workÔ¨Çows
‚óè
Observability
‚óè
SLAs
16
‚úÖ
Chunk size: 512 tokens
‚úÖ
Chunk overlap: 15% (~75 tokens)
‚úÖ
Top-k retrieval: 3-5 chunks
‚úÖ
Embedding dimensions: 768-1536
Default ConÔ¨Åguration 
(Works for 80% of cases):
17
‚úÖ
RAG = Open-book exam for AI - Retrieves external knowledge at query time
‚úÖ
Chunking is critical - Start with 512 tokens, 15% overlap, then iterate
‚úÖ
Hybrid search > vector-only - Combine vector and keyword search
‚úÖ
Start simple - Use Chroma + LangChain for learning, scale as needed
‚úÖ
Always evaluate - Track recall, precision, and answer quality
Key Takeaways
What You Should Remember
--- 1 part. RAG Intro.txt ---
 As we are starting with a rock topic, today we will have a small intro and then move on to the databases for Genii. Our speaker for today's both session is Maxim and we will start with intro. As I said, Maxim, you're very welcome to start. Hello. We're happy to see you all today. I will start from organizational topics as well. It depends on how fast we will move, potentially we will have some small break between sessions, but let's see. So today mainly we will focus exactly on the way how LLM can utilize external information for providing value to the users that basically utilize some systems and we will focus on the rock from the different perspective. First our focus and first our presentation will be around general rock, why do we need the rock and some more practical details that potentially will help you to navigate in this huge and complex world where you can find a lot of the tools for the different purpose, a lot of the databases, but you don't know where to start and maybe where and how to progress. So let's start maybe a bit from the introduction. So I'm a max right now at Siklom. I'm global head of cloud platforms and AI director. So I'm already nine years within Siklom. A long time ago I came here as a middle JavaScript engineer and went through the different stages of promotions and I was so lucky to have experience with different SDLC processes and stages that's right now I can jump much deeper not only into engineering topics in the developed topics, cloud topics and in a product topics as well. So from the start of chat GPT era I jumped to the AI direction as a self learner and I was amazed with the some of the experience that I got within the LLM and right now my main interest is to make a AI system as much reliable as it possible. That's why maybe you can find some of the topics that I did from the previous academies around the prompt engineering. This is where I started to adopt in LLM and try it to make them more reliable. And today we will go through like for this presentation we will go through three first topics. So we will focus on the definition and benefits. What we have from the rock some of the main use cases so from the practical life limitations and we'll cover a bit what tools you can use to speed up this process. So in general when we are talking about AI adaptation landscape so basically we have our base model and we want to direct that base model in our for example like domain or our use case or our application. So we need to start guiding or providing or maybe tuning the LLM and you always go from the simplest way of the tuning of the LLM to more complex. In some of the like articles you can find that even prompt engineering this is some  like tuning of the LLM because you are changing potential like behavior or narrowing direction of the LLM. And normally when you try to adopt LLM you are starting from that. The next stage and this is where we will focus mainly today for both of the sessions. It's retrieval augmented generation approach. This is where you already have some pipeline that helps you to add more extent and extend the knowledge and understanding of the LLM. The next stage it's like about the fine tuning mainly when you already need to adopt model in some way or like provide some way of the behavior of the model and for sure you can bring more knowledge to the model and the last one the biggest one where you have a combination of RUG and space-equipped fine tuning. And RUG for us the main approach we need to have like some fresh data or our proprietary data because all of the LLM have cutoff data like with the tool use they can get access to the more fresh data from the web but sometimes it's better and easier to provide our own data or especially if it's like proprietary and close data that you have access only you have access and they are not publicly. And it's one of the like most effective and cost effective way basically to let's say fine tune the model. And if we are starting from what exactly is RUG so this is the pattern that helps us to add additional knowledge to our model. In a simple way we have this like basic RUG pipeline where we have like first stage where we edit our knowledge and vectorize to some of the vector DB and then when our user use our system together with LLM basically first request is given to the vector database it's doing like some similarities or choosing a few of the chunks or like documents from that vector DB based on how many we set up and then LLM preparing the final response to our user. Why it's matters I already like partially explained but in addition it's quite often if you want to have like better accuracy in their responses because with the usage of the RUG approach you can put like exact data for the different topic that you want to utilize within your system when when user interacts with that. It's quite adaptable because you can quite fast change or update or add new knowledge to that RUG database much more cost effective and it's in addition it's increase efficiency especially when you need to provide like big amount of your like proprietary let's say data or like knowledge because for sure with the current size of the context window quite often you can feed a huge portion of the information to the LLM like with all of the knowledge that you want to utilize as a part of the output than when LLM is working on that data but a lot of the researchers show them that's bigger context windows that you utilize or like bigger prompt you you provided to the LLM you are getting much worse result because LLM quite often missing the exact like needed information in the middle and in addition you just provide in some noise together with that data like some of the words or information that's even not connected to your basically use case of the database of the communication with LLM. When we are going to deep dive so in the RUG pipeline in general we have two main stages but like when we go a bit to the details quite often and especially this information is missing when you when you're reading about the like RUG pipeline it's data preparation for that RUG pipeline because mainly it's focused on like the way of the chunking about the database you can use and how you put the data but it's quite important to start from the preparation of your data because I've mentioned for example when we provide a huge amount of the data to the LLM one of the big problem that's together with the data providing like water or noise that's just can navigate LLM when it use that like prompt information or text in the wrong direction so that's why quite often preparation of the of your data is quite important step as well then you have the step where you like preprocessing of the data like making like chunking a process because you can you can't just put all your 100 page documents to the vector database because it's probably huge and then LLM basically will get all of that 100 page documents as a prompt from the from the vector database so we have like different approaches how to split the information that comment with the with the documents and when we are talking about the documents the the information that's coming as an input can be like in many different formats because we are we are focusing mainly on the text formats but it can be audio it can be video it can be like images and different types like even chalice creep or python code basis then we after after the step when we split our data to the smaller pieces we are converting that smaller pieces to the numeric value and then that numeric value basically ingest to the vector database and we have a step of the retrieval it's one already our user of our system basically make the request and our system based on that request making the similarity search from the vector database and based on the results of that similarity search it's choosing like top key chunks like normally you are setting that's between like five to ten top results that you're grabbing from that vector database and then lm decide what part of the information from the chunk it should pick up to fulfill the intent of the request of the user and provide the output so chunking and approach to the chunking is a quite important part of this all of the rock system because like context window of the lm lm 6 limited for most of the lm's right now for sure we have quite a huge one even within the last jemenai versions they mentioned it can the context window can grow up to two million tokens but still the problem with missing information in the middle still persist with huge with huge context that you are providing to the model to the models even right now the situation is improving and mainly labs are working quite heavily to improve the retrieval and fighting and processing of the whole of the information that you provide into the lm but still this is the problem of reliability so to say and within the chunks you have like challenges to get the proper balance and if you provide like two large chunks if you if you split your document in a two large pieces then you will have as a result like not the best retrieval or a lot of the noise or water that you're providing as a part of the request the lm or if you put in it's too small then you are losing basically the main context of that information because it can be split it in a different part and it can be the case that based on the similarity search you will not get the full context of the information and lm can provide not reliable response you hear just representation of like three let's say base and main chunking strategies so fixed size semantic recursive but for sure you can find a lot of different approaches in terms of chunking so fixed size it's when we split our information basically in the fixed size like for example 512 tokens and we we are having like overlap and overlap basically help us to potentially preserve some context from the previous chunk and it help us to increase the potential quality of the chunks that we will have it is the simplest approach and for sure it has each each of the almost each of the approach or I would say like each approach has some cons and for for this for this approach this is the the problem that if you always have a standard size it can be the case that your chunk will be break in a mid of the sentence or will not have again still the main context that you need to preserve for this chunk and as a result the quality of the retrieval stage will be much lower the semantic one here quite often some ML system utilized and it's trying to split all of your information by the meaning and it has a much higher like accuracy and in most cases much higher quality of the retrieval stage because we are preserving in most cases like context that is needed to be in that chunk and when we are doing the similarity search and top results LLM is getting it's getting the proper context for all of the chunks that we picked up the problem for sure it's like slower and more expensive in terms of like compute and depends on the amount of the data that we have but in general the situation in this direction with the more compute and slower is getting better from the year to year and later on when we will have the next session I will give you some perspective on how fast things can can be changed with all of that tools that we utilize in right now for the agentic system and in in this ecosystem in general a recursive approach it's still mainly like coding coding approach without the utilization any of ML or AI system it's splitting of the information by separators with this approach you basically split we're chunking based on the structure of our document so we can identify the paragraphs we can identify the boundaries of the document and like for sure it's a bit more complex than fixed size and still we have that problem with the context because paragraphs still do not represent the full meaning of your text or your information and this is how that's three different approaches looks like so when we have the fixed size so we have two close to equal chunks and we have overlap of the parts of the sentence that we have quite often in our chunks when we have the recursive so recursive just split it our text based on the sentences and semantic it's more focused on what the meaning we have in each of the parts of our text that we provided right right now like for the last time a lot of the new approaches evolve in terms of chunking you can for sure hear about like agentic chunking while we utilize in the LLM and LLM provide to us more meaningful approach to this splitting on the chunks our information some embedding chunks when we utilize like exact embedding model and based on the the on the meaning of the text it help us to split the information better as well but they are coming with additional more extensive compute cost time and still we don't have like 100% reliable system that is the best approach for this splitting chunking or like embedding of the information so you don't need to implement like all of that that we mentioned like approaches to the chunks so some of the top libraries that's already fully implemented at least like basic approaches to the chunking so basically the length chain has has all of the basic approaches not only what I showed earlier in addition then they have more complex chunking approaches so it's a quiet known library like basically application framework that helping to build the agentic workflows and in under the hood it has a lot of additional connectors, side libraries that's helping us to build that agentic system and LLM index from the start it mostly was focused on the rock specific pipelines it has quite good performance it's data centric and it's just have a bit different approach how they implement this chunking strategy but you can find even the same chunking strategy implementation in both libraries so you can look there try them and most probably you will get quite good results from best in different chunk approaches for exact your cases. In terms of where mainly rock is using for sure we can we can talk about like from the perspective of general systems and from the perspective of the use cases when we're talking about perspective from the use cases so custom support, job search, detection and you name it while we have quite often updated system updated information with quite often updated system and mainly that system like proprietary notes fully publicly available especially but when we are talking about exact systems where it's mainly utilized it's like for the AI chatbots like for example if you have some support chatbots you are putting information as a to the rug like Q&A information or some support documents for your clients, search and discovery so combination of the keyword and vector search for better meaning of your searches, different compilates as an example it can be even how many of current systems like for example when we are talking about like cursor or other they basically use rug for the index indexation of the code base and much faster than finding the exact places in your code base for coding and helping to you and when we need the long context reasoning it's quite often utilized because again the quality of LLAM outputs is going down with the amount of the data that you feed as a part of their request so splitting and finding the exact parts of the information that you are going to provide is much better approach. So in terms of like common challenges still chunking and context to window as we as I mentioned about the problem with the chunking like if you provide in quite large chunks then it's mean that that's a quite large chunk then you feed to the model so we have the problem with the additional not needed information in exact that request then we have like retrieval quality so it based on the exact system that you utilize I mean like vector database or like approach to the data and latency and costs so for sure if we add to any system that we are developing if we add at some additional layer way you need to retrieve data or like make some filtering it's coming with the cost of latency. Some do some don'ts for the RAC system so quite often it's better to start from the simple approach even like from the simple base later on we will go a bit deeper on that. Use the metadata filtering in addition to the chunks most of the system provide possibility to add additional information and metadata like for example some category topic that's then it can be the part of your like filtering mechanism. Combining the vector and keyword search so hybrid approach and about that we will talk in our next session. Monitor the retrieval quality in addition to our like ingestion and our test we should prepare some pipeline that will double check the quality of our retrieval system. I will wait with the domain specific question so basically evaluation on that exact system that we are trying to build. To build like more reliable system not rely solely on the vector search so in addition you still have like your standard searches like keyword and we will discuss about some of the other approaches that's providing much better results when they work together as a hybrid approach and we're ranking approach as well. One of the biggest problems within the RAC this is the security and access control so it's quite complex to implement this sometimes but already some of the systems exist and we can just utilize them. Overload of the LM context so working on the balance of your chunks it shouldn't be quite big and for sure shouldn't be quite small because based on that your user of your system will get the result that's expected. Continuous update and do not skip evaluation framework. And here are not the three main chunking strategies but the systems that's or like libraries that you can utilize like for the scenarios of learning the main suggestion it's like LM chain because you can find a lot of the videos in YouTube it's quite extensive community around the LM chain and learning of the LM chain and that possibilities so basically you can utilize HROMA potentially sentence transformers or LAMA and some of the like key reasons it's you can like learn fundamentals and from the perspective or of risk-free like that stack that's I provided for the for the learning it's like fully open sourcing you can run just on your laptop but for sure you can if you do not use any data that shouldn't be sent to the models like chat GPT and stuff like that you can for sure utilize open EI or like Gemini embeddings for the for the easier approach and like as an MVP stage quite often we need just to change potential like HROMA and add a bit and other embeddings and for the more enterprise systems it can be the case that LM chain is not enough because it's quite good with the agentec workflows but has the problem with some agentec collaborative systems and finally come and again like open EI and Gemini embeddings one of the top right now in terms of quality the fault configuration so as a start for 80% of your cases chunk size if you are using like fixed chunk in approach so chunk size 512 tokens is enough and golden middle for sure based on your research you can identify different size of the chunks that work in for your cases better 15% of the chunk overlap top-caratriol I would say like 35 sometimes 10 and embedding dimensions between 700 and 1500s should be enough this data mainly on the research of the Nvidia and it's  like golden middle middle data and why you can start when you are building your chunk your rock systems but then within the evaluation and testing you can find your exact the best size for the chunks and for your rock system just few takeaways so rock this is the system that helping us to provide to the LLM external knowledge at the query time chunking is quite critical so I will not repeat about the exact numbers hybrid search is quite often better than like only vector only search start or always simple not only for the learning when you are starting prototyping all or development you can start from simpler systems not utilizing from the start the enterprise grade database is about what we will talk later today and quite important part it's evaluation of your rock pipeline not only like building but systems that will help you to make the evaluation of the quality of your pipeline is quite important time for the questions to be honest we only have three questions so yeah I believe we have already answered that but anyway how do we decide the chunk size for documents how do we decide chunk size for document you're starting basically from that chunk size that's I provided when we have like 512 tokens and with the slide in window of 15 percentage and this is why you are starting and then based on your like further experiments quite often when we are talking about like all AI or like ML development system you always have part as a research and then you based on the results that you are getting you are playing with the numbers so based on what you get and what you reviewed or maybe you already built the evaluation pipeline based on the numbers from the evaluation pipeline you can then just change your parameters and see if you are getting better results and then based on that researches based on that evaluations you can get the best number in your case in terms of what is the size and what is the approach to the chunks should be. Thank you Maxneum we have other question from Tolani she said that she found that LLAMs struggle was analyzing survey data she had started putting the documents as PDF and found that it works better so the questions are which chunk in strategy would an LLAM use for survey in PDF format versus CSV and which do you recommend. We will talk about this in our next session. Okay. So give that question for the next session in case we will not answer. Got it and the next question is RAC is just querying additional info from the internet and puts it in the context window of your model before providing the answer. Is it right? Almost except of internet this is our like separate database where we store in our data so this is like the domain pipeline but for sure we can say that this approach that's going to the internet and getting additional data this is a RAC system as well like a RAC system because we augmenting additional information to our LLAM but the base when we are talking about the RAC system it's mainly that we have some separate place where we store in some information that we want our system to utilize as a part of the of the answer of our LLAM. Okay thank you and the last one. Can I put RAC to my newly developed GPT and it will be very online and up to date. Is it? It's my newly developed GPT. Yep. Maybe a person can unmute and just say a few words what does it mean like newly developed GPT? Yes so it's my question so I just want to understand do I get it right about RACs? So if I have some LLAM whatever GPT or whatever and I don't want to retrain it I just want a new information like from the new database so it means that I can add this new shiny layer like RAC and then it's like chroma or whatever and it varies the data from this chroma before into the context window and just basically without retrain I will get all the all the information. Yes. Yes. Yes. Then it's fully correct. In a simple understanding maybe I just may clarify one point. Yes. So I believe the question was asked by Vasili so Vasili used like your I think you are oversimplifying it so in the first place you have to get your data into chroma so you have to get your data split it into chunks embedded, put it into chroma and then yes adding chroma and some RAC library on top of your GPT will work but you will get only your data you will not get all the new data available on the internet for that you would need to integrate with some other tools like internet search. Yes. I got it. Yes. Thank you so much for your questions. Maybe you have other or we can move on to the next part of the session. I have one question which is about the beginning part. It's just above my last message. Because without the beginning you mentioned that RAC injects into the prompt but did he mean the modern instead because I was confusing it's in the chat. Okay let's go. When we have the user query just to retrain. First step in most RAC systems where we based on that query our system find out the top parts of our data that we put to that database like five potential pieces of the information and then together with the prompt and that information our system making the request to the LLM and then LLM together with that information that we provided from the RAC database and the prompt that's like user query provide the answer to the user. Okay. So this will happen in the back end because on the front end I wouldn't see that. But I think of prompt as someone who is probably non-technical I think of whoever I've entered into the chat box. Yeah. For me just prompt this is anything that we are providing to the LLM all of the information. But yes I understand like just to show as a simple example of the RAC system as well. So when we create some projects for example in the chat GPT the project files that we are adding basically that projects then our RAC system because chat GPT just split that information to the chunks and then utilize that information when we are asking questions in our chat. It's the simple representation of RAC system and you can find all of that like GPs until you are providing the expected like BFF or instruction when you just provide the the files that you want GPs to utilize. It will be in addition to RAC system.
--- 2 part Databases for GenAI.txt ---
 session we can skip your slides. So in this session we will focus again we will reiterate a bit about the rug and we will go a bit more deeper in embedding, vectorization and other rug systems and possibilities and we should come to the state where we can even build quite big and reliable a rug system that will utilize rug. At this this slide I can skip. So we will focus on the foundation then more advanced rug patterns and some dosandons for production readiness and here we will cover even one of the questions that we had for the previous session. So on the foundation so what is in general like embeddings? So we have the problem with that example like words, machine learning algorithms, ML techniques and deep neural networks that's basically computer doesn't understand similarity of that words. Computers still understand only like numbers so it cannot help us to define just from the simple text are they similar or how to find. So basically like embeddings it's a transformation of the text to the numeric representation on the vector. So when we talk about like machine learning then for the for our embeddings we have like this representation of the numbers depends on the dimensions. So it's like on this exactly like image we have 500 36 dimensions so it's basically like graphs with the different points and in the dimension of this size we are we are putting the representation of this work on that graph. The same like for the ML algorithms, pizza and stuff like that. And if we are asking like about like machine learning for example then computer based on that vector representation and number representation can understand if like machine learning for example because we already transformed the representation into the numbers if it's close to some of the like information that we have right now in the vector system or it's far. So based on like close and far it's basically like vector databases when we set up like choose top five results providing to us with the answer from the five closest objects and then transform this to the text and we are basically getting that's that's text that's most closest to to the text from our request. In this case like for the machine learning ML algorithms is quite close because the distance 0.02 and if we just want to get only one like top element then we are getting ML algorithms. Pizza recipes is quite far so most probably system will not suggest this to us. And if we are going a bit like deeper to the exact like similarity search how it works. So we have our documents we already discussed about the rock pipeline so it's already transformed to the numerical representation from the embedded models and it stores somewhere here to the vector to the vector store. And we have like three documents and we fully indexed that documents and it's stored. And when we have a search and if we have like user query, query learn Python then based on that documents that we stored our system identified the distance for our request and in this case quite close somewhat close and far away. If we set up that we want to pick up two top results so in this case we will get Python tutorial and Java programming as well. And then LLAM may be make a decision that's to show only like Python tutorial but it can be the case that it will provide as a output to the user both like Python tutorial and Java programming because quality of the LLAM play quite significant role in this case as well. Quite often like similarity metrics that systems identified this is angle between the vectors sorry straight line just distance between them and in some cases some dot multiplication in that systems. So why is like embedding quality matters because basically it's mean that how reliable answer will our user of our system will get. And right now MTB metrics to be honest I don't remember exact like this full abbreviation but I provided the link to the leaderboard and you will see what exactly it's mean what's the abbreviation. So this is the approach how to how current embedding models evaluated on the quality of their basically embeddings and accuracy of their embeddings. And in the point of time when I was preparing this presentation a few of the main or like token embedding models basically from the Google and token AI and one of the open source knowledge search but right now I guess Chinese embedding models on a good spot there as well so you can try and play. And why exactly like embedding quality matters if we will see the different case studies or researchers you will still see that any embedding model or embedding system can provide 100% of the accuracy because still LLM or like similarity search can be like mistaken mistaken interpreter and one of the like best results that you can find like on the Google website for example it's research about the legal discovery so they put 1.4 million of different low documents with utilization of Gemini embedding and they achieve 87% accuracy and it's like for sure quite good number and with another like application mind lid they achieved 82% of three top results recall in terms of the matrix it's as well quite good result and in which what is good with the direction of embedding models and we discussed a bit when we discuss about chunking strategies that's when you utilize like additional ML or Genie or embedding models it's additional time for indexing and it's additional time for the compute so it's much slower Gemini embedding already showing like quite good results in terms of the speed so they tried to vectorize 100 emails on one of the study and they achieved 21 and half seconds for vectorization of all of that emails and just to give you perspective of how it fast and what is the how fast we are moving to improving all of these tools and systems the previous result was more than 200 seconds so the order of magnitude in terms of the speed increase for the vectorization in 10 times so that system much more like improved and we are going like on that speed and that changes with many models systems and tools in AI world because people experimenting companies putting a lot of effort in this race so to say and if we will talk a bit more about the databases exactly for the vectorization this is where I wanted to mention about the speed of changes as well because early I had this slide why we need to have like separate database for embeddings and that's like current databases like systems they have different algorithms they don't have everything that is needed for the embeddings that they quiet slow for the embeddings and it was but post-griest is moving with their extension they're moving in quite rapidly and catching up the speed question and especially amount of the operations basically like queries per second that they can handle and a lot of the researchers already show that post-griest is quite good alternative to the specialized even database for the RAC so you can see that's for example like faster than pinecon and for sure like cheaper and still open source and for quadrants it can handle much more request per second but it's on a huge amount of the data so they tested this on the 50 million embeddings and this is where like quadrants start degrading with the speed of working so potentially you can use just that database that you use that you use to and you just need to add additional extension for the embeddings until you can continue playing with your lovely post-griest but still we have the databases for the vectorization and they still play their own game and for what use case for what systems they have the best results and it was already mentioned about like chroma today and chroma is quite good when you start in some of the prototyping it's less rare used in some big production systems but they already have their cloud roma I didn't try and still they have some  like limitations so chroma database quite good when you start in end prototyping because it's quite easy to start you just pipian style chroma and almost everything is working so minimal configuration and you are starting almost like immediately with that it still has quite good additional functionality like for example metadata filtering this is the additional information to your embeddings that you can provide as an example I've already mentioned for example like category of the document that you are feeding if you have like many of the documents and chunks for example in the metadata you can still additionally provide information about what exact document from from from from chunk is and different like multiple collection support when we are moving already like to the stage of potential MVP or already quite big production system then we need to have more reliable solution and here we can look in the direction of quadrant or for example like postgres with the pg vector extension and on the previous slide you you saw that quadrant not so good on 50 million vectors in terms of queries per second but when it's in the in the measure between like one to 10 million it's really quite quite fast and can handle a lot of the requests like for example for one million it's 626 queries per second and it's really quite fast and for that if you have that range of the vectors and you need to have quiet rapid embedding quadrant is a good choice in general but if you have like billions of vectors then this is the question about another type of the system and Milsvus and they have this embedding database in a cloud it calls Zleen it's already fully designed for quiet huge vector systems and it doesn't make sense to use this database for like much smaller systems if you have under 10 million vectors so do not recommend because that's that data and that's systems that going from the left to right it's then just harder like more complex to support setup and configure so that's why you are starting from the simplest one just for the rapid start and when your system is growing you are moving to the let's say next one and issue them just fulfill their purples and about like wrap patterns wrap patterns in addition to the embeddings because we discussed about the embedding vectorization it's not only one pattern or approach for the rock systems in addition we can have this problem with the multi-hop reasoning with the connection and in the cases where we have like a bit more wake request and we need to identify the connection for that request in most cases rock traditional vector rock systems they are failing and in example that I provided and the problem that we need to have like connection built from our request that user asks like marketing budget compliance GDPR and basically Europe and this is the the connections and this is exactly one we need already to utilize graph system and graph rock in addition and it helps to increase reliability of our system together with LLM and quite often this is like a representation of the graph that we have and when we our user making the request it's just like getting some of the information from the request and learning the different connections and based on the connection that we have in the rock representation it can provide much better answer if we do not have like explicit information and one chunk of the information that we are grabbing from our vector system is not enough. main players for the graph rock solutions so now for Jay, Falcon, Tiger, Graph, Mamm Graph and ArangoDB mainly we are playing like with the now for Jay because you can easily like install it on the York laptop and I guess it's one of the most popular solution I would say. the next problem that's quite often come in the PDF processing problem so in the PDF we can have images in the PDF we can have tables and maybe some formulas so it's quiet complex documents to parse for the LLM and in the traditional rock pipelines we have like OCR objects so basically the ML system or it can be like LLM that can look exactly on the PDF and then provide in the text in the text view describe what it sees on the PDF and then we can just like vectorize it quite easily. still we have like issues with the tables quite often and here like some OCR again can help some other approaches, captions so instead of like using complex trivial system and that's relying on OCR because they quite often like failing with that we can just use some embedding model that can understand what it sees so just embed the image and one of the interesting approach to solve this question for the last time it's quite like you let's say approach it's called poly library changes so they instead of like using OCR just for the describing what information exists on the PDF and then vectorize that information they have like vision language model for that and image representation just split to the patches and then that patches basically embed to the model and when we have the retrieval then we utilize that embedding model and still like visual language model to give the answer what we have from the images and this approach shows much better results in terms of like a rock system retrieval of the information from the system and it showed better result like 15% it's approximately then standard to three-wall system with the OCR that we have so to answer on the question quite often when we utilize the PDFs when we have a PDFs as documents we utilize the OCR for describing of that documents and then we store the information but this approach with the call poly showing quite interesting results and maybe it's something that will be used much more often in the future as the as a part of this type of rock systems but still mostly OCR approach used and a bit about like agentech rock because in AI right now everything changed to the agentech rock and for sure this is quite interesting and quite reliable approach so in the standard rock we have query and that query go into the embedding model then we query the embedding vector databases then vector databases provide to us like candidates like top candidates and then our query so our request plus candidates that our vector database provided goes to the llem llem then process all of that information that we put and then provide to our user the answer when we have the agentech rock we have type of like rotor agent llem and then within the two set that's available for that llem it's choose where to to roll that's request so to the rock search some external APIs or take in control of the over the world and then only providing the output message here we will have a bit more information on that so agentech patterns and mainly like query routing requests some of the the composition and self correction approaches in the agentech rock system so when the user make the requests what the latest on AI regulation so llem rotor detect like latest then we will utilize the tool road to web search so that's why I said like in general if we are saying that we add just additional tool like web search yes it's to some extent rock system and like when we talk about the query the composition so we have a compare our q3 to industry and predict q4 and first of all we have the the composition of that request that's agents our llem bricks like our q3 matrix then we need to find this information in our internal database or a vector system industry q3 benchmarks potentially we don't have this information it's publicly available then web search and q4 factors again goes to the internal DB but again it's can go to the web search as well and the search pattern its self correction so we first have a retrieval then we get in the documents then our agents based on some identified our like threshold measure this and if a score is like lower than our threshold then it can even like rewrite the query and then basically like a retry again this approach with the retrieval grade docs and it can be cycled few times this self correction and when it's it's to use well when we have like why it's complex worries and in cases if everything else that we discussed earlier failed and you needs to have like more higher accuracy if we have when we shouldn't use this it's for the simple retrieval and if our top quality attribute for our system is latency because you understand that this agentic systems when we have llem and needs to increase the quality of their results the time of these types of the request is growing i guess we will need maybe a xanae guess our time is and yes we are a bit out of time if you have a time you can continue and also colleagues if you have a little bit of 10 minutes you can continue this session. I will try to finish this like in five minutes and then we will have five minutes for for the questions I just will not stop quite deeply on each of the slide just few words and what is the information important from that in addition to increase the quality of our rock systems and why I mentioned earlier that you shouldn't rely only on the vector search in addition you can utilize like best much search this is what bm25 it doesn't have semantic understanding but it's calculated the number of words that it finds like the same word that it's finding the different documents and can provide based on that statistic statistical calculation what the documents we should become and on the researchers when we have like a hybrid system approach in terms of the search vector plus this bm25 it increases accuracy for a bit more than 10 percentage and dcgs3 this is like tops we recalls basically tops we candidates that we saw earlier on the diagram and when we add the rerunter in addition it's even increased to 37.2 percentage rerunkin is additional layer that we add into the system and we are grabbing from our vector system or from our database a bit more results so we are we are grabbing instead of like 510 candidates we are grabbing 100 candidates and then our rerunkin system try to understand what the best candidates for us and then provide us a result to the lm like 5 or 10 or maybe 3 of them and production readiness so basically what to do don't hybrid search is the best we discussed in addition about like a genetic search we discussed about the graph and where it's the most useful and what the databases you can utilize so based on your case based on what you need to achieve you can use any use any of that tools metadata filtering it's quite helpful especially when you need to have like metadata in general like a story of the metadata quite helpful especially when you need to have their sources seated like for example when you make the request and get not only some information from your documents in addition the citations from what documents that information is evaluation of your system embedding three indexing and evaluation of the system quite often because you are making their updates and you can continue improving your system so don't it's like opposite of the dos so just quite helpful for you to not forget what you should do for the production systems and thank you so we have one question in our chat does embedding library I use for for my rock must match embedding library I used for training my lm embedding library for training your lm can you maybe give a bit more details yes so before training lm as I understand I need to embed like all the text and everything and do the embedding yeah when you have a rock system you do not train your lm like fine tunes your lm yeah it's not about I mean like when I train my lm I use some embedding for example from from Facebook or Google do I have to use the same embedding library for my rock I mean as I understand like to put something in rock you also need to do embedding do you mean when you are storing the documents to the vector database and when you are retrieving the documents from the database do you need to use the same embedding library when it generates the knowledge and the knowledge based okay and the version and everything should be the same and identical with the live with the lm yes sometimes especially like from one provider they are compatible one and another but you need to double check this information but general answer is yes good you thanks other question we have metrics like n gcg or recall require ground truth answers should they come from humans this one you are preparing you're preparing in most cases yes because you are preparing the expected results on your request so yeah mainly it's human prepared metrics thank you Maxim colleagues other questions maybe you have I have a question thank you so thank you for the session and in one of the latest slides you mentioned that we have to evaluate often and I think this is the most complex task when it comes to building a i-based solution so my question is whether we'll have any session any session that will explain how to build this evaluations for our systems or overall like agentic systems so the question is about evaluations I will double check maybe in the next sessions that we will have on the rock enterprise productized maybe we will have a decision on evaluation or we will discuss with our colleagues that we should potentially like add this as a part of our education because right now what what I see I don't see like exact evaluation in the session but potentially it's a part of some of the next sessions thank you why I said not only because preparation of the data it's not so easy as well without evaluation you just don't know whether you did good job at preparing your data or not so you can be building a system for like weeks or months but if you do not have any metrics to check its accuracy and performance then it was for nothing so as always human in the loop can save you from that but yes depends on the scalability of your system and all of that parameters yes I agree but in general what can I say it's not so easy still a question for the evaluation in general with with all of this alarm systems because they are like they are not like deterministic systems and still you need to identify the proper way how you can like even identify the top three that's a system should recall so different approaches in most the most the approaches like some another LLN utilized if you are not utilizing the people to to prepare the data so it's still not finalized question even for in general for the AI industry I would say
--- pdf_text.txt ---
--- Databases for GenAI.pdf ---
1
Place for photo
Databases for 
GenAI
Embeddings, Vector Databases & Production 
Patterns
October 2025
2
Speaker
Maksym Lypivskyi
Head of Cloud Platforms & 
AI Director
‚óè
9 years at Ciklum driving large-scale cloud and 
software delivery initiatives
‚óè
3 years specializing in AI
‚óè
Core interest: making AI systems reliable, 
production-ready, and business-impactful
3
Agenda
01
02
03
Foundations
Advanced RAG patterns
Production readiness
4
Foundations
5
What Are Embeddings?
The Problem: Computers don't understand text meaning
"machine learning 
algorithms"
"ML techniques"
"deep neural networks"
Are these similar? How 
do we compute that?
6
How Similarity Search Works
The Vector Search Process
Documents ‚Üí Embedding Model ‚Üí Store vectors
Doc 1: "Python tutorial" ‚Üí [0.2, -0.4, 0.6, ...] ‚Üí Store
Doc 2: "Java programming" ‚Üí [0.3, -0.5, 0.5, ...] ‚Üí Store
Doc 3: "Cooking recipes" ‚Üí [-0.8, 0.9, -0.2, ...] ‚Üí Store
1. INDEXING
2. SEARCHING
 User Query: "learn Python" ‚Üí [0.21, -0.42, 0.58, ...]
 Compute distances to ALL stored vectors:
 - Distance to Doc 1: 0.03 ‚úì (very close!)
 - Distance to Doc 2: 0.15 (somewhat close)
 - Distance to Doc 3: 0.91 (far away)
Similarity 
Metrics
- Cosine similarity: Angle between vectors (most common)
- Euclidean distance: Straight-line distance
- Dot product: Vector multiplication
7
Embedding Quality Matters
üèõ EVERLAW (Legal 
Discovery - 1.4M 
documents)
https://huggingface.co/spaces/mteb/leaderboard
Model
Provider
MTEB
Dims
Price/1M
stella_en_1.5B_v5
NovaSearch
71.54
1024-8192
Free (OSS)
gemini-embedding-001
Google
68.32
3072
$0.15
text-embedding-3-large
OpenAI
64.6
3072
$0.13
text-embedding-3-small
OpenAI
62.3
1536
$0.02
üí¨ MINDLID
üìß INTERACTION CO 
(Email Assistant - 100 
emails)
87% 
accuracy
82% 
3 recall
21.45s 
embedd
8
PostgreSQL Revolution
9
Architecture & Performance
- Rust-based for maximum speed
- 626 QPS at 99.5% recall (1M vectors)
- Sub-3ms p95 latency
- Open-source (Apache 2.0 license)
Memory Optimization: 
- Binary Quantization: 40x memory reduction - 
Scalar quantization 
- Product quantization
Advanced Capabilities:
- Native hybrid search (dense + sparse 
vectors)
- Advanced payload Ô¨Åltering
- Distributed architecture for horizontal 
scaling
- RESTful and gRPC APIs
From MVP to Scale
Mid Size High-Performance Vector Search
Architecture & Performance
- Built for massive scale (billions of vectors)
- 2,098 QPS at 100% recall (10M vectors)
- Sub-10ms latency on performance 
conÔ¨Ågurations
- Open-source (Apache 2.0 license)
Memory Optimization: 
- Int8 compression: 75% memory savings
- RabitQ 1-bit quantization
- Multiple quantization strategies
Advanced Capabilities:
- GPU acceleration support
- Up to 100,000 collections per cluster
- Distributed architecture with query node 
separation
- Multiple distance metrics and index types 
(HNSW, IVF, DiskANN)
Architecture & Performance
- Embedded database (runs in-process with 
your application)
- 112 QPS at 10M vectors
- Python and JavaScript SDKs
- Open-source (Apache 2.0 license)
Memory Optimization: 
- In-memory and persistent storage modes
- Optimized for datasets under 1M vectors
- Simple distance functions (cosine, L2, inner 
product)
Advanced Capabilities:
- Zero-conÔ¨Åguration setup (pip install 
chromadb)
- Metadata Ô¨Åltering
- Multiple collection support
10
Advanced RAG patterns
11
11
The Multi-Hop Reasoning Problem
Complex Query:
"What are the regulatory implications of our Q3 marketing 
strategy for the European market?"
Traditional RAG fails
Retrieves isolated chunks:
üÜá
- "Q3 marketing strategy increased 
social media spend"
üÜá
- "European market regulations 
overview" 
üÜá
- "GDPR compliance requirements"
Problem: Can't connect the dots!
Marketing ‚Üí Budget ‚Üí Compliance ‚Üí GDPR ‚Üí Europe
This is where we need Graph RAG
12
12
GraphRAG Solution
13
13
The PDF Processing Problem
üÜá
OCR ‚Üí Errors with complex layouts
üÜá
Layout detection ‚Üí Misses table structures
üÜá
Figure captioning ‚Üí Expensive specialized 
models
üÜá
Chunking ‚Üí Loses visual context
üÜá
Embed text only ‚Üí No visual information
Traditional RAG Pipeline for PDFs
Result: 40-60% information loss on 
visual documents
14
14
ColPali Revolution
Treat Pages as Images (No OCR!)
Architecture
‚óè
PaliGemma-3B Vision Language Model
‚óè
32√ó32 patches = 1,024 patch 
embeddings per page
‚óè
ColBERT late interaction matching
15
15
What Makes RAG "Agentic"
16
16
Agentic Patterns
Query Decomposition
Complex: "Compare our Q3 to 
industry and predict Q4"
Agent breaks down:
‚óè
"Our Q3 metrics" ‚Üí Internal DB
‚óè
"Industry Q3 benchmarks" ‚Üí 
Web Search
‚óè
"Q4 factors" ‚Üí Internal DB
Then synthesizes all results
Self-Correction
1.
Retrieval
2.
Grade docs
3.
Score < threshold?
4.
Rewrite query 
5.
Retry
6.
Grade again
Query Routing
User: "What's the latest on AI 
regulation?"
Router: "latest" detected ‚Üí Route 
to Web Search ‚úì
User: "What's our Q3 marketing 
budget?" 
Router: "our" = internal ‚Üí Route to 
Vector DB ‚úì
‚úÖ
Use When: Queries span sources, complexity varies
üÜá
Don't Use: Latency-critical (<500ms SLA), simple retrieval
17
17
Hybrid Search + Reranking Power
Vector-only: Misses exact matches, acronyms
BM25-only: No semantic understanding
Anthropic Contextual Retrieval 
Study:
- Baseline failure rate: 5.7%
- Hybrid + Reranker: 1.9%
- 67% reduction in retrieval failures
2 Stage Architecture Rerank:
Stage 1: 
Similarity search ‚Üí Top 100 
Stage 2: 
Re-ranking pipeline ‚Üí Top 5-10 
Microsoft Azure AI Search 
Benchmarks:
- Vector-only: 43.8 NDCG@3
- Hybrid: 48.4 NDCG@3 (+10.5%)
- Hybrid + Reranker: 60.1 NDCG@3 
(+37.2%)
‚úÖ
Use When: Queries span sources, complexity varies
üÜá
Don't Use: Latency-critical (<500ms SLA), simple retrieval
18
Production readiness
19
19
Don‚Äôts
üÜá
1. Vector-only search (use hybrid)
üÜá
2. Ignore access controls (data leakage = 
lawsuit)
üÜá
3. Overload context window (keep <50%)
üÜá
4. Skip evaluation frameworks (doesn't 
scale)
üÜá
5. Neglect data freshness (stale = wrong 
answers)
Do‚Äôs
‚úÖ
1. Use Hybrid Search (vector + BM25) as 
baseline
‚úÖ
2. Implement metadata Ô¨Åltering 
(security-critical)
‚úÖ
 Ô¨Ålter = {"department": "Ô¨Ånance", 
"access_level": user.role}
‚úÖ
3. Monitor retrieval quality (recall@k, 
NDCG)
‚úÖ
 Tools: TruLens, LangSmith, DeepEval
‚úÖ
4. Keep embeddings fresh (re-index on 
doc changes)
‚úÖ
5. Evaluate systematically (not 
"vibes-based")
Production Best Practices
--- RAG Intro.pdf ---
1
Place for photo
RAG
Intro
October 2025
2
Speaker
Maksym Lypivskyi
Head of Cloud Platforms & 
AI Director
‚óè
9 years at Ciklum driving large-scale cloud and 
software delivery initiatives
‚óè
3 years specializing in AI
‚óè
Core interest: making AI systems reliable, 
production-ready, and business-impactful
3
Agenda
01
02
03
DeÔ¨Ånition & BeneÔ¨Åts
Use cases
Limitations
4
The AI Adaptation Landscape
Where Does RAG Fit in Your AI Strategy?
‚óè
Need fresh, proprietary knowledge
‚óè
Don't want to retrain models
‚óè
Cost-effective scaling
Our focus today
5
What is RAG?
Retrieval‚Äëaugmented generation (RAG) is a 
pattern that augments an LLM by retrieving 
relevant information from external sources at 
query time and injecting it into the prompt.
6
Why RAG Matters?
‚óè
Fresh Additional Knowledge. RAG lets 
you ground answers in recent 
documents, internal wikis, or databases 
without retraining models.
‚óè
Better Accuracy. By retrieving 
authoritative evidence, the model 
generates more accurate answers and 
can cite sources.
‚óè
Adaptable. Effectively handles novel and 
niche queries that weren't in the model's 
training data.
‚óè
Increases EfÔ¨Åciency. RAG grounds 
prompts with smaller, targeted chunks of 
information that streamline retrieval and 
generation.
7
RAG Pipeline Deep Dive
8
Chunking - The Critical Decision
Why Chunking Matters
LLMs have limited context windows (32k-200k 
tokens). Large documents must be divided into 
smaller pieces.
The Challenge:
‚óè
Too large ‚Üí loses specificity, poor retrieval
‚óè
Too small ‚Üí loses context, fragmented 
information
9
Three Main Chunking Strategies
Strategy
How It Works
Pros
Cons
When to Use
Fixed-Size
Split at 512 
tokens, 15% 
overlap
Simple, fast, 
predictable
May break 
mid-sentence
General purpose, 
starting point
Semantic
Use ML to 
identify coherent 
units
Preserves 
meaning, high 
accuracy
More compute, 
slower
Technical docs, 
complex content
Recursive
Split using 
separators (\n\n, 
\n, space) 
repeatedly until 
desired size
Respects 
structure, better 
boundaries
More complex 
than Ô¨Åxed-size
Documents with 
headings, 
paragraphs
10
10
Chunking Strategy Comparison
Fixed-Size
Recursive
Semantic
11
11
Using Chunking Libraries
You Don't Need to Build from Scratch
LangChain
LlamaIndex
Broad LLM application 
framework
Modular workÔ¨Çows where 
chunking is one piece of the 
puzzle
‚óèFlexible TextSplitters
‚óèEasy integration with agents
‚óèPart of larger system
RAG-speciÔ¨Åc pipeline
High-performance, 
data-centric retrieval systems
‚óèSophisticated NodeParsers
‚óèProduces optimized "Nodes"
‚óèBuilt for ingestion/retrieval
12
12
RAG real world use cases
‚óè
AI Chatbots: RAG provides accurate answers from internal 
knowledge bases (e.g., support wikis, legal documents). OpenAI 
emphasises that RAG is valuable when the content is not part of 
the base model‚Äôs knowledge .
‚óè
Search & discovery: Search systems combine keyword and 
vector search to surface relevant documents in e‚Äëcommerce, 
research and legal discovery.
‚óè
AI Copilots: Tools like Supabase AI Copilots use vector 
databases to ground responses in proprietary data and maintain 
multi‚Äëtenant isolation .
‚óè
Long‚Äëcontext reasoning: Databricks‚Äô long‚Äëcontext benchmark 
shows that Google‚Äôs Gemini 2.5 models can maintain consistent 
performance on RAG tasks up to two million tokens (longer than 
most models), whereas OpenAI‚Äôs GPT 5 models achieve 
state‚Äëof‚Äëthe‚Äëart accuracy up to 128k tokens .
13
13
Common Challenges
‚óè
Chunking & context windows: If chunks are poorly deÔ¨Åned, the retrieved information may miss critical context 
or include too much irrelevant text. Research by Analytics Vidhya notes that Ô¨Åxed‚Äësize chunking can break 
context while semantic‚Äëbased chunking preserves meaning but requires more compute .
‚óè
Model context length: Models can only ingest a Ô¨Ånite number of tokens. Databricks‚Äô benchmark observed that 
performance of LLMs like Llama‚Äë3.1 and GPT‚Äë4 starts to degrade when context windows exceed 32‚Äì64 k tokens 
.
‚óè
Retrieval quality: The quality of the vector database and retrieval algorithm determines recall. Missing relevant 
documents leads to hallucinated answers.
‚óè
Latency & cost: Large vector databases and embedding models can be expensive and introduce latency.
14
Don‚Äôts
üÜá
Rely solely on vector search
üÜá
Ignore security and access controls
üÜá
Overload the LLM context window
üÜá
Neglect continuous updates
üÜá
Skip evaluation frameworks
Do‚Äôs
‚úÖ
Start simple, iterate based on metrics
‚úÖ
Use metadata Ô¨Åltering (product, language, 
permissions)
‚úÖ
Combine vector + keyword search (hybrid approach)
‚úÖ
Monitor retrieval quality (recall@k, precision)
‚úÖ
Keep embeddings synchronized with documents
‚úÖ
Evaluate with domain-speciÔ¨Åc questions
Do‚Äôs and don‚Äôts for RAG
15
15
Three Main Chunking Strategies
Scenario
Stack
Key Reason
Learning
LangChain + Chroma + 
sentence-transformers + Ollama
‚óè
Learn fundamentals
‚óè
Risk-free 
‚óè
Runs on laptop
MVP
LangChain + Qdrant (self-host) + 
OpenAI/Gemini embeddings + GPT-4o-mini
Professional quality at 
startup budget
Enterprise
LangGraph + LangSmith + Pinecone + 
OpenAI/Gemini + GPT-4
‚óè
Agentic workÔ¨Çows
‚óè
Observability
‚óè
SLAs
16
‚úÖ
Chunk size: 512 tokens
‚úÖ
Chunk overlap: 15% (~75 tokens)
‚úÖ
Top-k retrieval: 3-5 chunks
‚úÖ
Embedding dimensions: 768-1536
Default ConÔ¨Åguration 
(Works for 80% of cases):
17
‚úÖ
RAG = Open-book exam for AI - Retrieves external knowledge at query time
‚úÖ
Chunking is critical - Start with 512 tokens, 15% overlap, then iterate
‚úÖ
Hybrid search > vector-only - Combine vector and keyword search
‚úÖ
Start simple - Use Chroma + LangChain for learning, scale as needed
‚úÖ
Always evaluate - Track recall, precision, and answer quality
Key Takeaways
What You Should Remember