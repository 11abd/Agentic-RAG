
--- Databases for GenAI.pdf ---

1
Place for photo
Databases for 
GenAI
Embeddings, Vector Databases & Production 
Patterns
October 2025

2
Speaker
Maksym Lypivskyi
Head of Cloud Platforms & 
AI Director
â—
9 years at Ciklum driving large-scale cloud and 
software delivery initiatives
â—
3 years specializing in AI
â—
Core interest: making AI systems reliable, 
production-ready, and business-impactful

3
Agenda
01
02
03
Foundations
Advanced RAG patterns
Production readiness

4
Foundations

5
What Are Embeddings?
The Problem: Computers don't understand text meaning
"machine learning 
algorithms"
"ML techniques"
"deep neural networks"
Are these similar? How 
do we compute that?

6
How Similarity Search Works
The Vector Search Process
Documents â†’ Embedding Model â†’ Store vectors
Doc 1: "Python tutorial"     â†’ [0.2, -0.4, 0.6, ...] â†’ Store
Doc 2: "Java programming"    â†’ [0.3, -0.5, 0.5, ...] â†’ Store
Doc 3: "Cooking recipes"     â†’ [-0.8, 0.9, -0.2, ...] â†’ Store
1. INDEXING
2. SEARCHING
  User Query: "learn Python"  â†’ [0.21, -0.42, 0.58, ...]
  Compute distances to ALL stored vectors:
  - Distance to Doc 1: 0.03 âœ“ (very close!)
  - Distance to Doc 2: 0.15 (somewhat close)
  - Distance to Doc 3: 0.91 (far away)
Similarity 
Metrics
- Cosine similarity: Angle between vectors (most common)
- Euclidean distance: Straight-line distance
- Dot product: Vector multiplication

7
Embedding Quality Matters
ğŸ› EVERLAW (Legal 
Discovery - 1.4M 
documents)
https://huggingface.co/spaces/mteb/leaderboard
Model
Provider
MTEB
Dims
Price/1M
stella_en_1.5B_v5
NovaSearch
71.54
1024-8192
Free (OSS)
gemini-embedding-001
Google
68.32
3072
$0.15
text-embedding-3-large
OpenAI
64.6
3072
$0.13
text-embedding-3-small
OpenAI
62.3
1536
$0.02
ğŸ’¬ MINDLID
ğŸ“§ INTERACTION CO 
(Email Assistant - 100 
emails)
87% 
accuracy
82% 
3 recall
21.45s 
embedd

8
PostgreSQL Revolution

9
Architecture & Performance
- Rust-based for maximum speed
- 626 QPS at 99.5% recall (1M vectors)
- Sub-3ms p95 latency
- Open-source (Apache 2.0 license)
Memory Optimization: 
- Binary Quantization: 40x memory reduction - 
Scalar quantization 
- Product quantization
Advanced Capabilities:
- Native hybrid search (dense + sparse 
vectors)
- Advanced payload ï¬ltering
- Distributed architecture for horizontal 
scaling
- RESTful and gRPC APIs
From MVP to Scale
Mid Size High-Performance Vector Search
Architecture & Performance
- Built for massive scale (billions of vectors)
- 2,098 QPS at 100% recall (10M vectors)
- Sub-10ms latency on performance 
conï¬gurations
- Open-source (Apache 2.0 license)
Memory Optimization: 
- Int8 compression: 75% memory savings
- RabitQ 1-bit quantization
- Multiple quantization strategies
Advanced Capabilities:
- GPU acceleration support
- Up to 100,000 collections per cluster
- Distributed architecture with query node 
separation
- Multiple distance metrics and index types 
(HNSW, IVF, DiskANN)
Architecture & Performance
- Embedded database (runs in-process with 
your application)
- 112 QPS at 10M vectors
- Python and JavaScript SDKs
- Open-source (Apache 2.0 license)
Memory Optimization: 
- In-memory and persistent storage modes
- Optimized for datasets under 1M vectors
- Simple distance functions (cosine, L2, inner 
product)
Advanced Capabilities:
- Zero-conï¬guration setup (pip install 
chromadb)
- Metadata ï¬ltering
- Multiple collection support

10
Advanced RAG patterns

11
11
The Multi-Hop Reasoning Problem
Complex Query:
"What are the regulatory implications of our Q3 marketing 
strategy for the European market?"
Traditional RAG fails
Retrieves isolated chunks:
ğŸ†‡
- "Q3 marketing strategy increased 
social media spend"
ğŸ†‡
- "European market regulations 
overview"  
ğŸ†‡
- "GDPR compliance requirements"
Problem: Can't connect the dots!
Marketing â†’ Budget â†’ Compliance â†’ GDPR â†’ Europe
This is where we need Graph RAG

12
12
GraphRAG Solution

13
13
The PDF Processing Problem
ğŸ†‡
OCR â†’ Errors with complex layouts
ğŸ†‡
Layout detection â†’ Misses table structures
ğŸ†‡
Figure captioning â†’ Expensive specialized 
models
ğŸ†‡
Chunking â†’ Loses visual context
ğŸ†‡
Embed text only â†’ No visual information
Traditional RAG Pipeline for PDFs
Result: 40-60% information loss on 
visual documents

14
14
ColPali Revolution
Treat Pages as Images (No OCR!)
Architecture
â—
PaliGemma-3B Vision Language Model
â—
32Ã—32 patches = 1,024 patch 
embeddings per page
â—
ColBERT late interaction matching

15
15
What Makes RAG "Agentic"

16
16
Agentic Patterns
Query Decomposition
Complex: "Compare our Q3 to 
industry and predict Q4"
Agent breaks down:
â—
"Our Q3 metrics" â†’ Internal DB
â—
"Industry Q3 benchmarks" â†’ 
Web Search
â—
"Q4 factors" â†’ Internal DB
Then synthesizes all results
Self-Correction
1.
Retrieval
2.
Grade docs
3.
Score < threshold?
4.
Rewrite query 
5.
Retry
6.
Grade again
Query Routing
User: "What's the latest on AI 
regulation?"
Router: "latest" detected â†’ Route 
to Web Search âœ“
User: "What's our Q3 marketing 
budget?"  
Router: "our" = internal â†’ Route to 
Vector DB âœ“
âœ…
Use When: Queries span sources, complexity varies
ğŸ†‡
Don't Use: Latency-critical (<500ms SLA), simple retrieval

17
17
Hybrid Search + Reranking Power
Vector-only: Misses exact matches, acronyms
BM25-only: No semantic understanding
Anthropic Contextual Retrieval 
Study:
- Baseline failure rate: 5.7%
- Hybrid + Reranker: 1.9%
- 67% reduction in retrieval failures
2 Stage Architecture Rerank:
Stage 1: 
Similarity search â†’ Top 100 
Stage 2: 
Re-ranking pipeline â†’ Top 5-10 
Microsoft Azure AI Search 
Benchmarks:
- Vector-only: 43.8 NDCG@3
- Hybrid: 48.4 NDCG@3 (+10.5%)
- Hybrid + Reranker: 60.1 NDCG@3 
(+37.2%)
âœ…
Use When: Queries span sources, complexity varies
ğŸ†‡
Don't Use: Latency-critical (<500ms SLA), simple retrieval

18
Production readiness

19
19
Donâ€™ts
ğŸ†‡
1. Vector-only search (use hybrid)
ğŸ†‡
2. Ignore access controls (data leakage = 
lawsuit)
ğŸ†‡
3. Overload context window (keep <50%)
ğŸ†‡
4. Skip evaluation frameworks (doesn't 
scale)
ğŸ†‡
5. Neglect data freshness (stale = wrong 
answers)
Doâ€™s
âœ…
1. Use Hybrid Search (vector + BM25) as 
baseline
âœ…
2. Implement metadata ï¬ltering 
(security-critical)
âœ…
   ï¬lter = {"department": "ï¬nance", 
"access_level": user.role}
âœ…
3. Monitor retrieval quality (recall@k, 
NDCG)
âœ…
   Tools: TruLens, LangSmith, DeepEval
âœ…
4. Keep embeddings fresh (re-index on 
doc changes)
âœ…
5. Evaluate systematically (not 
"vibes-based")
Production Best Practices


--- RAG Intro.pdf ---

1
Place for photo
RAG
Intro
October 2025

2
Speaker
Maksym Lypivskyi
Head of Cloud Platforms & 
AI Director
â—
9 years at Ciklum driving large-scale cloud and 
software delivery initiatives
â—
3 years specializing in AI
â—
Core interest: making AI systems reliable, 
production-ready, and business-impactful

3
Agenda
01
02
03
Deï¬nition & Beneï¬ts
Use cases
Limitations

4
The AI Adaptation Landscape
Where Does RAG Fit in Your AI Strategy?
â—
Need fresh, proprietary knowledge
â—
Don't want to retrain models
â—
Cost-effective scaling
Our focus today

5
What is RAG?
Retrievalâ€‘augmented generation (RAG) is a 
pattern that augments an LLM by retrieving 
relevant information from external sources at 
query time and injecting it into the prompt.

6
Why RAG Matters?
â—
Fresh Additional Knowledge. RAG lets 
you ground answers in recent 
documents, internal wikis, or databases 
without retraining models.
â—
Better Accuracy. By retrieving 
authoritative evidence, the model 
generates more accurate answers and 
can cite sources.
â—
Adaptable. Effectively handles novel and 
niche queries that weren't in the model's 
training data.
â—
Increases Efï¬ciency. RAG grounds 
prompts with smaller, targeted chunks of 
information that streamline retrieval and 
generation.

7
RAG Pipeline Deep Dive

8
Chunking - The Critical Decision
Why Chunking Matters
LLMs have limited context windows (32k-200k 
tokens). Large documents must be divided into 
smaller pieces.
The Challenge:
â—
Too large â†’ loses specificity, poor retrieval
â—
Too small â†’ loses context, fragmented 
information

9
Three Main Chunking Strategies
Strategy
How It Works
Pros
Cons
When to Use
Fixed-Size
Split at 512 
tokens, 15% 
overlap
Simple, fast, 
predictable
May break 
mid-sentence
General purpose, 
starting point
Semantic
Use ML to 
identify coherent 
units
Preserves 
meaning, high 
accuracy
More compute, 
slower
Technical docs, 
complex content
Recursive
Split using 
separators (\n\n, 
\n, space) 
repeatedly until 
desired size
Respects 
structure, better 
boundaries
More complex 
than ï¬xed-size
Documents with 
headings, 
paragraphs

10
10
Chunking Strategy Comparison
Fixed-Size
Recursive
Semantic

11
11
Using Chunking Libraries
You Don't Need to Build from Scratch
LangChain
LlamaIndex
Broad LLM application 
framework
Modular workï¬‚ows where 
chunking is one piece of the 
puzzle
â—Flexible TextSplitters
â—Easy integration with agents
â—Part of larger system
RAG-speciï¬c pipeline
High-performance, 
data-centric retrieval systems
â—Sophisticated NodeParsers
â—Produces optimized "Nodes"
â—Built for ingestion/retrieval

12
12
RAG real world use cases
â—
AI Chatbots: RAG provides accurate answers from internal 
knowledge bases (e.g., support wikis, legal documents).  OpenAI 
emphasises that RAG is valuable when the content is not part of 
the base modelâ€™s knowledge .
â—
Search & discovery: Search systems combine keyword and 
vector search to surface relevant documents in eâ€‘commerce, 
research and legal discovery.
â—
AI Copilots: Tools like Supabase AI Copilots use vector 
databases to ground responses in proprietary data and maintain 
multiâ€‘tenant isolation .
â—
Longâ€‘context reasoning: Databricksâ€™ longâ€‘context benchmark 
shows that Googleâ€™s Gemini 2.5 models can maintain consistent 
performance on RAG tasks up to two million tokens (longer than 
most models), whereas OpenAIâ€™s GPT 5 models achieve 
stateâ€‘ofâ€‘theâ€‘art accuracy up to 128k tokens .

13
13
Common Challenges
â—
Chunking & context windows: If chunks are poorly deï¬ned, the retrieved information may miss critical context 
or include too much irrelevant text.  Research by Analytics Vidhya notes that ï¬xedâ€‘size chunking can break 
context while semanticâ€‘based chunking preserves meaning but requires more compute .
â—
Model context length: Models can only ingest a ï¬nite number of tokens.  Databricksâ€™ benchmark observed that 
performance of LLMs like Llamaâ€‘3.1 and GPTâ€‘4 starts to degrade when context windows exceed 32â€“64 k tokens 
.
â—
Retrieval quality: The quality of the vector database and retrieval algorithm determines recall.  Missing relevant 
documents leads to hallucinated answers.
â—
Latency & cost: Large vector databases and embedding models can be expensive and introduce latency.

14
Donâ€™ts
ğŸ†‡
Rely solely on vector search
ğŸ†‡
Ignore security and access controls
ğŸ†‡
Overload the LLM context window
ğŸ†‡
Neglect continuous updates
ğŸ†‡
Skip evaluation frameworks
Doâ€™s
âœ…
Start simple, iterate based on metrics
âœ…
Use metadata ï¬ltering (product, language, 
permissions)
âœ…
Combine vector + keyword search (hybrid approach)
âœ…
Monitor retrieval quality (recall@k, precision)
âœ…
Keep embeddings synchronized with documents
âœ…
Evaluate with domain-speciï¬c questions
Doâ€™s and donâ€™ts for RAG

15
15
Three Main Chunking Strategies
Scenario
Stack
Key Reason
Learning
LangChain + Chroma + 
sentence-transformers + Ollama
â—
Learn fundamentals
â—
Risk-free 
â—
Runs on laptop
MVP
LangChain + Qdrant (self-host) + 
OpenAI/Gemini embeddings + GPT-4o-mini
Professional quality at 
startup budget
Enterprise
LangGraph + LangSmith + Pinecone + 
OpenAI/Gemini + GPT-4
â—
Agentic workï¬‚ows
â—
Observability
â—
SLAs

16
âœ…
Chunk size: 512 tokens
âœ…
Chunk overlap: 15% (~75 tokens)
âœ…
Top-k retrieval: 3-5 chunks
âœ…
Embedding dimensions: 768-1536
Default Conï¬guration 
(Works for 80% of cases):

17
âœ…
RAG = Open-book exam for AI - Retrieves external knowledge at query time
âœ…
Chunking is critical - Start with 512 tokens, 15% overlap, then iterate
âœ…
Hybrid search > vector-only - Combine vector and keyword search
âœ…
Start simple - Use Chroma + LangChain for learning, scale as needed
âœ…
Always evaluate - Track recall, precision, and answer quality
Key Takeaways
What You Should Remember
