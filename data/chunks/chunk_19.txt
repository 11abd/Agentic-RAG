higher like accuracy and in most cases much higher quality of the retrieval stage because we are preserving in most cases like context that is needed to be in that chunk and when we are doing the similarity search and top results LLM is getting it's getting the proper context for all of the chunks that we picked up the problem for sure it's like slower and more expensive in terms of like compute and depends on the amount of the data that we have but in general the situation in this direction with the more compute and slower is getting better from the year to year and later on when we will have the next session I will give you some perspective on how fast things can can be changed with all of that tools that we utilize in right now for the agentic system and in in this ecosystem in general a recursive approach it's still mainly like coding coding approach without the utilization any of ML or AI system it's splitting of the information by separators with this approach you basically split we're chunking based on the structure of our document so we can identify the paragraphs we can identify the boundaries of the document and like for sure it's a bit more complex than fixed size and still we have that problem with the context because paragraphs still do not represent the full meaning of your text or your information and this is how that's three