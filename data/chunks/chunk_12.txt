about the fine tuning mainly when you already need to adopt model in some way or like provide some way of the behavior of the model and for sure you can bring more knowledge to the model and the last one the biggest one where you have a combination of RUG and space-equipped fine tuning. And RUG for us the main approach we need to have like some fresh data or our proprietary data because all of the LLM have cutoff data like with the tool use they can get access to the more fresh data from the web but sometimes it's better and easier to provide our own data or especially if it's like proprietary and close data that you have access only you have access and they are not publicly. And it's one of the like most effective and cost effective way basically to let's say fine tune the model. And if we are starting from what exactly is RUG so this is the pattern that helps us to add additional knowledge to our model. In a simple way we have this like basic RUG pipeline where we have like first stage where we edit our knowledge and vectorize to some of the vector DB and then when our user use our system together with LLM basically first request is given to the vector database it's doing like some similarities or choosing a few of the chunks or like documents from that vector DB based on how many