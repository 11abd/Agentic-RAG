proprietary data and maintain multiâ€‘tenant isolation . â— Longâ€‘context reasoning: Databricksâ€™ longâ€‘context benchmark shows that Googleâ€™s Gemini 2.5 models can maintain consistent performance on RAG tasks up to two million tokens (longer than most models), whereas OpenAIâ€™s GPT 5 models achieve stateâ€‘ofâ€‘theâ€‘art accuracy up to 128k tokens . 13 13 Common Challenges â— Chunking & context windows: If chunks are poorly deï¬ned, the retrieved information may miss critical context or include too much irrelevant text. Research by Analytics Vidhya notes that ï¬xedâ€‘size chunking can break context while semanticâ€‘based chunking preserves meaning but requires more compute . â— Model context length: Models can only ingest a ï¬nite number of tokens. Databricksâ€™ benchmark observed that performance of LLMs like Llamaâ€‘3.1 and GPTâ€‘4 starts to degrade when context windows exceed 32â€“64 k tokens . â— Retrieval quality: The quality of the vector database and retrieval algorithm determines recall. Missing relevant documents leads to hallucinated answers. â— Latency & cost: Large vector databases and embedding models can be expensive and introduce latency. 14 Donâ€™ts ğŸ†‡ Rely solely on vector search ğŸ†‡ Ignore security and access controls ğŸ†‡ Overload the LLM context window ğŸ†‡ Neglect continuous updates ğŸ†‡ Skip evaluation frameworks Doâ€™s âœ… Start simple, iterate based on metrics âœ… Use metadata ï¬ltering (product, language, permissions) âœ… Combine vector + keyword search (hybrid approach) âœ… Monitor retrieval quality (recall@k, precision) âœ… Keep embeddings synchronized with documents âœ… Evaluate with domain-speciï¬c questions Doâ€™s and donâ€™ts for RAG 15 15 Three Main Chunking Strategies Scenario Stack Key