business-impactful 3 Agenda 01 02 03 Deﬁnition & Beneﬁts Use cases Limitations 4 The AI Adaptation Landscape Where Does RAG Fit in Your AI Strategy? ● Need fresh, proprietary knowledge ● Don't want to retrain models ● Cost-effective scaling Our focus today 5 What is RAG? Retrieval‑augmented generation (RAG) is a pattern that augments an LLM by retrieving relevant information from external sources at query time and injecting it into the prompt. 6 Why RAG Matters? ● Fresh Additional Knowledge. RAG lets you ground answers in recent documents, internal wikis, or databases without retraining models. ● Better Accuracy. By retrieving authoritative evidence, the model generates more accurate answers and can cite sources. ● Adaptable. Effectively handles novel and niche queries that weren't in the model's training data. ● Increases Efﬁciency. RAG grounds prompts with smaller, targeted chunks of information that streamline retrieval and generation. 7 RAG Pipeline Deep Dive 8 Chunking - The Critical Decision Why Chunking Matters LLMs have limited context windows (32k-200k tokens). Large documents must be divided into smaller pieces. The Challenge: ● Too large → loses specificity, poor retrieval ● Too small → loses context, fragmented information 9 Three Main Chunking Strategies Strategy How It Works Pros Cons When to Use Fixed-Size Split at 512 tokens, 15% overlap Simple, fast, predictable May break mid-sentence General purpose, starting point Semantic Use ML to identify coherent units Preserves meaning, high accuracy More compute, slower Technical docs, complex content Recursive Split using separators (\n\n, \n, space) repeatedly until desired