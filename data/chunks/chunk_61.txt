and embedding models can be expensive and introduce latency. 14 Donâ€™ts ðŸ†‡ Rely solely on vector search ðŸ†‡ Ignore security and access controls ðŸ†‡ Overload the LLM context window ðŸ†‡ Neglect continuous updates ðŸ†‡ Skip evaluation frameworks Doâ€™s âœ… Start simple, iterate based on metrics âœ… Use metadata ï¬ltering (product, language, permissions) âœ… Combine vector + keyword search (hybrid approach) âœ… Monitor retrieval quality (recall@k, precision) âœ… Keep embeddings synchronized with documents âœ… Evaluate with domain-speciï¬c questions Doâ€™s and donâ€™ts for RAG 15 15 Three Main Chunking Strategies Scenario Stack Key Reason Learning LangChain + Chroma + sentence-transformers + Ollama â— Learn fundamentals â— Risk-free â— Runs on laptop MVP LangChain + Qdrant (self-host) + OpenAI/Gemini embeddings + GPT-4o-mini Professional quality at startup budget Enterprise LangGraph + LangSmith + Pinecone + OpenAI/Gemini + GPT-4 â— Agentic workï¬‚ows â— Observability â— SLAs 16 âœ… Chunk size: 512 tokens âœ… Chunk overlap: 15% (~75 tokens) âœ… Top-k retrieval: 3-5 chunks âœ… Embedding dimensions: 768-1536 Default Conï¬guration (Works for 80% of cases): 17 âœ… RAG = Open-book exam for AI - Retrieves external knowledge at query time âœ… Chunking is critical - Start with 512 tokens, 15% overlap, then iterate âœ… Hybrid search > vector-only - Combine vector and keyword search âœ… Start simple - Use Chroma + LangChain for learning, scale as needed âœ… Always evaluate - Track recall, precision, and answer quality Key Takeaways What You Should Remember