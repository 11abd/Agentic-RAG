the exact places in your code base for coding and helping to you and when we need the long context reasoning it's quite often utilized because again the quality of LLAM outputs is going down with the amount of the data that you feed as a part of their request so splitting and finding the exact parts of the information that you are going to provide is much better approach. So in terms of like common challenges still chunking and context to window as we as I mentioned about the problem with the chunking like if you provide in quite large chunks then it's mean that that's a quite large chunk then you feed to the model so we have the problem with the additional not needed information in exact that request then we have like retrieval quality so it based on the exact system that you utilize I mean like vector database or like approach to the data and latency and costs so for sure if we add to any system that we are developing if we add at some additional layer way you need to retrieve data or like make some filtering it's coming with the cost of latency. Some do some don'ts for the RAC system so quite often it's better to start from the simple approach even like from the simple base later on we will go a bit deeper on that. Use the metadata filtering in addition to the chunks most of the system provide