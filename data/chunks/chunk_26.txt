can if you do not use any data that shouldn't be sent to the models like chat GPT and stuff like that you can for sure utilize open EI or like Gemini embeddings for the for the easier approach and like as an MVP stage quite often we need just to change potential like HROMA and add a bit and other embeddings and for the more enterprise systems it can be the case that LM chain is not enough because it's quite good with the agentec workflows but has the problem with some agentec collaborative systems and finally come and again like open EI and Gemini embeddings one of the top right now in terms of quality the fault configuration so as a start for 80% of your cases chunk size if you are using like fixed chunk in approach so chunk size 512 tokens is enough and golden middle for sure based on your research you can identify different size of the chunks that work in for your cases better 15% of the chunk overlap top-caratriol I would say like 35 sometimes 10 and embedding dimensions between 700 and 1500s should be enough this data mainly on the research of the Nvidia and it's like golden middle middle data and why you can start when you are building your chunk your rock systems but then within the evaluation and testing you can find your exact the best size for the chunks and for your rock system just few takeaways