so fixed size semantic recursive but for sure you can find a lot of different approaches in terms of chunking so fixed size it's when we split our information basically in the fixed size like for example 512 tokens and we we are having like overlap and overlap basically help us to potentially preserve some context from the previous chunk and it help us to increase the potential quality of the chunks that we will have it is the simplest approach and for sure it has each each of the almost each of the approach or I would say like each approach has some cons and for for this for this approach this is the the problem that if you always have a standard size it can be the case that your chunk will be break in a mid of the sentence or will not have again still the main context that you need to preserve for this chunk and as a result the quality of the retrieval stage will be much lower the semantic one here quite often some ML system utilized and it's trying to split all of your information by the meaning and it has a much higher like accuracy and in most cases much higher quality of the retrieval stage because we are preserving in most cases like context that is needed to be in that chunk and when we are doing the similarity search and top results LLM is getting it's getting the proper context