we can identify the boundaries of the document and like for sure it's a bit more complex than fixed size and still we have that problem with the context because paragraphs still do not represent the full meaning of your text or your information and this is how that's three different approaches looks like so when we have the fixed size so we have two close to equal chunks and we have overlap of the parts of the sentence that we have quite often in our chunks when we have the recursive so recursive just split it our text based on the sentences and semantic it's more focused on what the meaning we have in each of the parts of our text that we provided right right now like for the last time a lot of the new approaches evolve in terms of chunking you can for sure hear about like agentic chunking while we utilize in the LLM and LLM provide to us more meaningful approach to this splitting on the chunks our information some embedding chunks when we utilize like exact embedding model and based on the the on the meaning of the text it help us to split the information better as well but they are coming with additional more extensive compute cost time and still we don't have like 100% reliable system that is the best approach for this splitting chunking or like embedding of the information so you don't need to implement like all of