and most probably you will get quite good results from best in different chunk approaches for exact your cases. In terms of where mainly rock is using for sure we can we can talk about like from the perspective of general systems and from the perspective of the use cases when we're talking about perspective from the use cases so custom support, job search, detection and you name it while we have quite often updated system updated information with quite often updated system and mainly that system like proprietary notes fully publicly available especially but when we are talking about exact systems where it's mainly utilized it's like for the AI chatbots like for example if you have some support chatbots you are putting information as a to the rug like Q&A information or some support documents for your clients, search and discovery so combination of the keyword and vector search for better meaning of your searches, different compilates as an example it can be even how many of current systems like for example when we are talking about like cursor or other they basically use rug for the index indexation of the code base and much faster than finding the exact places in your code base for coding and helping to you and when we need the long context reasoning it's quite often utilized because again the quality of LLAM outputs is going down with the amount of the data that you feed as a part of their request