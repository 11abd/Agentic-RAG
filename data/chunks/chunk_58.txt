Intro October 2025 2 Speaker Maksym Lypivskyi Head of Cloud Platforms & AI Director ● 9 years at Ciklum driving large-scale cloud and software delivery initiatives ● 3 years specializing in AI ● Core interest: making AI systems reliable, production-ready, and business-impactful 3 Agenda 01 02 03 Deﬁnition & Beneﬁts Use cases Limitations 4 The AI Adaptation Landscape Where Does RAG Fit in Your AI Strategy? ● Need fresh, proprietary knowledge ● Don't want to retrain models ● Cost-effective scaling Our focus today 5 What is RAG? Retrieval‑augmented generation (RAG) is a pattern that augments an LLM by retrieving relevant information from external sources at query time and injecting it into the prompt. 6 Why RAG Matters? ● Fresh Additional Knowledge. RAG lets you ground answers in recent documents, internal wikis, or databases without retraining models. ● Better Accuracy. By retrieving authoritative evidence, the model generates more accurate answers and can cite sources. ● Adaptable. Effectively handles novel and niche queries that weren't in the model's training data. ● Increases Efﬁciency. RAG grounds prompts with smaller, targeted chunks of information that streamline retrieval and generation. 7 RAG Pipeline Deep Dive 8 Chunking - The Critical Decision Why Chunking Matters LLMs have limited context windows (32k-200k tokens). Large documents must be divided into smaller pieces. The Challenge: ● Too large → loses specificity, poor retrieval ● Too small → loses context, fragmented information 9 Three Main Chunking Strategies Strategy How It Works Pros Cons When to Use Fixed-Size Split at 512