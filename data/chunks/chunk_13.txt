vectorize to some of the vector DB and then when our user use our system together with LLM basically first request is given to the vector database it's doing like some similarities or choosing a few of the chunks or like documents from that vector DB based on how many we set up and then LLM preparing the final response to our user. Why it's matters I already like partially explained but in addition it's quite often if you want to have like better accuracy in their responses because with the usage of the RUG approach you can put like exact data for the different topic that you want to utilize within your system when when user interacts with that. It's quite adaptable because you can quite fast change or update or add new knowledge to that RUG database much more cost effective and it's in addition it's increase efficiency especially when you need to provide like big amount of your like proprietary let's say data or like knowledge because for sure with the current size of the context window quite often you can feed a huge portion of the information to the LLM like with all of the knowledge that you want to utilize as a part of the output than when LLM is working on that data but a lot of the researchers show them that's bigger context windows that you utilize or like bigger prompt you you provided to the LLM you are getting much worse result