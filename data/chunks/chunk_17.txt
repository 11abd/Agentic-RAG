the middle still persist with huge with huge context that you are providing to the model to the models even right now the situation is improving and mainly labs are working quite heavily to improve the retrieval and fighting and processing of the whole of the information that you provide into the lm but still this is the problem of reliability so to say and within the chunks you have like challenges to get the proper balance and if you provide like two large chunks if you if you split your document in a two large pieces then you will have as a result like not the best retrieval or a lot of the noise or water that you're providing as a part of the request the lm or if you put in it's too small then you are losing basically the main context of that information because it can be split it in a different part and it can be the case that based on the similarity search you will not get the full context of the information and lm can provide not reliable response you hear just representation of like three let's say base and main chunking strategies so fixed size semantic recursive but for sure you can find a lot of different approaches in terms of chunking so fixed size it's when we split our information basically in the fixed size like for example 512 tokens and we we are having like overlap and overlap basically help